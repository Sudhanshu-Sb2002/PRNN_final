{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1>Blood Classsifcation </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "from PRNN_final.LinearRegression import regression_classifier\n",
    "from PRNN_final.knn import knn_naive"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets import the required data and preprocess it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def normalize_images(images: np):\n",
    "    new_images = np.zeros((images.shape[0], images.shape[1] * images.shape[2] * images.shape[3]))\n",
    "    for i in range(images.shape[0]):\n",
    "        new_images[i] = images[i].flatten()\n",
    "    new_images = new_images.astype(np.float32)\n",
    "    new_images /= 255.0\n",
    "    return new_images\n",
    "\n",
    "\n",
    "def load_input(inputpath):\n",
    "    folder = np.load(inputpath)\n",
    "    files = folder.files\n",
    "    # print(files)\n",
    "    # First we load the images and the labels\n",
    "    un_train_images = folder['train_images.npy']\n",
    "    un_val_images = folder['val_images.npy']\n",
    "    un_test_images = folder['test_images.npy']\n",
    "    train_labels = folder['train_labels.npy']\n",
    "    val_labels = folder['val_labels.npy']\n",
    "    test_labels = folder['test_labels.npy']\n",
    "    # Then we normalize the images\n",
    "    train_images = normalize_images(un_train_images)\n",
    "    val_images = normalize_images(un_val_images)\n",
    "    test_images = normalize_images(un_test_images)\n",
    "    return train_images, val_images, test_images, train_labels, val_labels, test_labels\n",
    "\n",
    "def one_hot_encoding(labels):\n",
    "    one_hot_labels = numpy.zeros((labels.shape[0], 8))\n",
    "    for i in range(labels.shape[0]):\n",
    "        one_hot_labels[i][labels[i]] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "def plotter(x_graph, AUCscores, ACCscores, F1scores, title, xlabel,*,first=False):\n",
    "\n",
    "    plt.plot(x_graph, AUCscores,alpha=0.5)\n",
    "    plt.plot(x_graph, ACCscores,alpha=0.5 )\n",
    "    plt.plot(x_graph, F1scores,alpha=0.5)\n",
    "    plt.scatter(x_graph, AUCscores,s=6)\n",
    "    plt.scatter(x_graph, ACCscores,s=6)\n",
    "    plt.scatter(x_graph, F1scores,s=6)\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.ylim(0.5,1)\n",
    "    plt.grid(True, which='major')\n",
    "    plt.grid(True, which='minor',linestyle='--',linewidth=0.2)\n",
    "    plt.minorticks_on()\n",
    "\n",
    "    if first:\n",
    "        plt.figlegend(['AUC', 'ACC', 'F1'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "path=\"bloodmnist.npz\"\n",
    "train_images, val_images, test_images, train_labels, val_labels, test_labels = load_input(path)\n",
    "best_methods_AUC=dict()\n",
    "best_methods_ACC=dict()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> KNN Classifier </h2>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def knn_naive(X_train, Y_train, X_test, *, k=5, metric=2):\n",
    "    # First we difeine a small function to return the k smallet elements of an array\n",
    "    def k_smallest(arr, k):\n",
    "        x = []\n",
    "        min = np.inf\n",
    "        pos_min = -1\n",
    "        for i in range(k):\n",
    "            for i in range(len(arr)):\n",
    "                if arr[i] < min and i not in x:\n",
    "                    min = arr[i]\n",
    "                    pos_min = i\n",
    "            if pos_min != -1:\n",
    "                x.append(pos_min)\n",
    "                min = np.inf\n",
    "                pos_min = -1\n",
    "        return x\n",
    "\n",
    "    Y_test_predicted = np.zeros((X_test.shape[0], 1))\n",
    "    if k == 0:\n",
    "        return Y_test_predicted\n",
    "    for i in range(X_test.shape[0]):\n",
    "        distances = np.linalg.norm(np.abs(X_train - X_test[i, :]), ord=metric, axis=1)\n",
    "\n",
    "        nearest = k_smallest(distances, k)\n",
    "        topk_y = [i[0] for i in Y_train[nearest[:k]]]\n",
    "        Y_test_predicted[i] = [np.argmax(np.bincount(topk_y))]\n",
    "\n",
    "    return Y_test_predicted"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k_max=6\n",
    "no_of_tests=4\n",
    "x_graph = np.zeros((no_of_tests,k_max))\n",
    "AUCscores = np.zeros(x_graph.shape)\n",
    "ACCscores = np.zeros(x_graph.shape)\n",
    "F1scores = np.zeros(x_graph.shape)\n",
    "\n",
    "for j in range(no_of_tests):\n",
    "    metric=j\n",
    "    if j==0:\n",
    "        metric =-math.inf\n",
    "    elif j==no_of_tests-1:\n",
    "        metric = math.inf\n",
    "    for k in range(0, k_max):\n",
    "        test_pred = knn_naive(train_images, train_labels, test_images, k=k, metric=metric)\n",
    "        x_graph[j,k] = k\n",
    "        F1scores[j,k] =metrics.f1_score(test_labels,test_pred,average='weighted')\n",
    "        ACCscores[j,k] =metrics.accuracy_score(test_labels,test_pred)\n",
    "        #AUCscores[j,k] =metrics.roc_auc_score(test_labels,test_pred,multi_class='ovr')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.dpi'] = 300\n",
    "for j in range(no_of_tests):\n",
    "    metric=j\n",
    "    if j==0:\n",
    "        metric =-math.inf\n",
    "    elif j==no_of_tests-1:\n",
    "        metric = math.inf\n",
    "    plt.subplot(2,2,j+1)\n",
    "    plotter(x_graph[j],AUCscores[j],ACCscores[j],F1scores[j],str(metric)+\" norm\",\"K value\",first=(j==0))\n",
    "    plt.xticks(np.linspace(0, k_max, 5))\n",
    "\n",
    "plt.suptitle(\"KNN for binary classification with different metrics\")\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.figlegend(['AUC', 'ACC', 'F1'])\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count=0\n",
    "print(\"best metrics according to AUC\")\n",
    "for i in np.argsort(AUCscores,axis=None,)[-3:]:\n",
    "    best_metric,best_k =int( i//k_max), int(i%k_max    )\n",
    "    best_score = AUCscores[int(best_metric)][int(best_k)]\n",
    "    if best_metric==0:\n",
    "        best_metric =-math.inf\n",
    "    elif best_metric==no_of_tests-1:\n",
    "        best_metric = math.inf\n",
    "    print('\\t'+str(3-count)+\" best metric is \", best_metric,\"(k=\",best_k,\")with a score of \",best_score)\n",
    "    if 3-count==1:\n",
    "        best_methods_AUC[\"KNN\" + \"(metric=\" + str(best_metric) + \", k=\" + str(best_k)]=best_score\n",
    "    count+=1\n",
    "print(\"best metrics according to ACC\")\n",
    "count=0\n",
    "for i in np.argsort(ACCscores,axis=None,)[-3:]:\n",
    "    best_metric,best_k =int( i//k_max), int(i%k_max    )\n",
    "    best_score = ACCscores[int(best_metric)][int(best_k)]\n",
    "    if best_metric==0:\n",
    "        best_metric =-math.inf\n",
    "    elif best_metric==no_of_tests-1:\n",
    "        best_metric = math.inf\n",
    "    print('\\t'+str(3-count)+\" best metric is \", best_metric,\"(k=\",best_k,\")with a score of \",best_score)\n",
    "    if 3-count==1:\n",
    "        best_methods_AUC[\"KNN\" + \"(metric=\" + str(best_metric) + \", k=\" + str(best_k)]=best_score\n",
    "    count+=1\n",
    "%reset_selective -f AUCscores ACCscores F1scores x_graph k_max no_of_tests metric j k test_pred best_metric best_k best_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2>Linear Regression </h2>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_labels1 = one_hot_encoding(train_labels)\n",
    "test_labels1 = one_hot_encoding(test_labels)\n",
    "\n",
    "k_max=6\n",
    "no_of_tests=4\n",
    "x_graph = np.zeros((no_of_tests,k_max))\n",
    "AUCscores = np.zeros(x_graph.shape)\n",
    "ACCscores = np.zeros(x_graph.shape)\n",
    "F1scores = np.zeros(x_graph.shape)\n",
    "\n",
    "for j in range(no_of_tests):\n",
    "    metric=j\n",
    "    if j==0:\n",
    "        metric =-math.inf\n",
    "    elif j==no_of_tests-1:\n",
    "        metric = math.inf\n",
    "    for k in range(0, k_max):\n",
    "        y_predonehot = regression_classifier(train_images, train_labels1, test_images, lambda_hyper=-0.01*math.exp(k*10))\n",
    "        test_pred=_pred = np.argmax(y_predonehot, axis=1)\n",
    "        x_graph[j,k] = k\n",
    "        F1scores[j,k] =metrics.f1_score(test_labels,test_pred,average='weighted')\n",
    "        ACCscores[j,k] =metrics.accuracy_score(test_labels,test_pred)\n",
    "        #AUCscores[j,k] =metrics.roc_auc_score(test_labels,test_pred,multi_class='ovr')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plotter(x_graph, AUCscores, ACCscores, F1scores, \"Linear CLassifier with L2 regulariser\", \"Regulariser ( $\\lambda $) \")\n",
    "plt.legend(['AUC', 'ACC', 'F1']);\n",
    "count = 0\n",
    "print(\"best metrics according to AUC\")\n",
    "for i in np.argsort(AUCscores, axis=None, )[-3:]:\n",
    "\n",
    "    best_score = AUCscores[i]\n",
    "\n",
    "    print('\\t' + str(3 - count) + \" best regulariser is  \", round(x_graph[i], 3), \"With score\", round(best_score, 3))\n",
    "    if 3 - count == 1:\n",
    "        best_methods_AUC[\"Least squares with regulariser \", x_graph[i]] = best_score\n",
    "    count += 1\n",
    "print(\"best metrics according to ACC\")\n",
    "count = 0\n",
    "\n",
    "for i in np.argsort(ACCscores, axis=None, )[-3:]:\n",
    "\n",
    "    best_score = ACCscores[i]\n",
    "\n",
    "    print('\\t' + str(3 - count) + \" best regulariser is  \", round(x_graph[i], 3), \"With score\", round(best_score, 3))\n",
    "    if 3 - count == 1:\n",
    "        best_methods_ACC[\"Least squares with regulariser \", x_graph[i]] = best_score\n",
    "    count += 1\n",
    "\n",
    "% reset_selective -f AUCscores ACCscores F1scores x_graph k_max test_pred  best_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def regressionstuff(train_images, train_labels0, test_images, test_labels0, km):\n",
    "    train_labels1 = one_hot_encoding(train_labels0)\n",
    "    test_labels1 = one_hot_encoding(test_labels0)\n",
    "    x_graph = numpy.zeros(km)\n",
    "    AUCscores = numpy.zeros(x_graph.size)\n",
    "    ACCscores = numpy.zeros(x_graph.size)\n",
    "    F1scores = numpy.zeros(x_graph.size)\n",
    "    for k in range(km):\n",
    "        y_predonehot = regression_classifier(train_images, train_labels1, test_images, lambda_hyper=-0.01*math.exp(k*10))\n",
    "\n",
    "        y_pred = np.argmax(y_predonehot, axis=1)\n",
    "        x_graph[k] = k\n",
    "        ACCscores[k] = basic_classification_accuraccy(test_labels0, y_pred)\n",
    "    plotter(x_graph, None, ACCscores, None, 'Regression', 'K')\n",
    "\n",
    "\n",
    "def basic_classification_accuraccy(y_true, y_pred):\n",
    "    correct = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            correct += 1\n",
    "    return correct / len(y_true)\n",
    "\n",
    "\n",
    "def k_means_evaluator(train_images, test_images, train_labels, test_labels, *, kmax=5):\n",
    "    x_graph = numpy.zeros(kmax)\n",
    "    ACCscores = numpy.zeros(x_graph.size)\n",
    "\n",
    "    for k in range(0, kmax):\n",
    "        Y_test = knn_naive(train_images, train_labels, test_images, k=k, metric=2)\n",
    "        x_graph[k] = k\n",
    "        # AUCscores[k] = EVALUATOR.getAUC(test_labels, Y_test, 'binary-class')\n",
    "        # ACCscores[k] = EVALUATOR.getACC(test_labels, Y_test, 'binary-class')\n",
    "        ACCscores[k] = basic_classification_accuraccy(test_labels, Y_test)\n",
    "\n",
    "    plotter(x_graph, None, ACCscores, None, 'K-means based multiclass classifier', 'k')\n",
    "\n",
    "\n",
    "def main(inputpath):\n",
    "\n",
    "\n",
    "    # now we train the data using different methods\n",
    "    # k_means_evaluator(train_images, test_images, train_labels, test_labels, kmax=10)\n",
    "\n",
    "    regressionstuff(train_images, train_labels, test_images, test_labels, km=5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}